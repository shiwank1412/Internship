{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f905b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:-1 Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the header tags (h1 to h6) using the find_all method\n",
    "header_tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "# Extract the text from the header tags and store them in a list\n",
    "header_texts = [tag.get_text() for tag in header_tags]\n",
    "\n",
    "# Create a data frame using pandas\n",
    "df = pd.DataFrame(header_texts, columns=[\"Header\"])\n",
    "\n",
    "# Display the data frame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e720f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question :-2 Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d82a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:-3 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51bc3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ans:- A)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151669b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans:- b)\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans:- C) \n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:-4  Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8ff86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans:- A)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape Top 10 ODI teams in women's cricket\n",
    "url_teams = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response_teams = requests.get(url_teams)\n",
    "soup_teams = BeautifulSoup(response_teams.content, \"html.parser\")\n",
    "\n",
    "teams_data = []\n",
    "table_teams = soup_teams.find(\"table\", class_=\"table\")\n",
    "rows_teams = table_teams.find_all(\"tr\")\n",
    "\n",
    "for row in rows_teams[1:11]:\n",
    "  team_name = row.find(\"span\", class_=\"u-hide-phablet\").text.strip()\n",
    "  matches = row.find_all(\"td\")[2].text.strip()\n",
    "  points = row.find_all(\"td\")[3].text.strip()\n",
    "  rating = row.find_all(\"td\")[4].text.strip()\n",
    "  teams_data.append([team_name, matches, points, rating])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a1b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans:- B)\n",
    "\n",
    "# Scrape Top 10 women's ODI Batting players\n",
    "\n",
    "url_batting = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response_batting = requests.get(url_batting)\n",
    "soup_batting = BeautifulSoup(response_batting.content, \"html.parser\")\n",
    "\n",
    "batting_data = []\n",
    "table_batting = soup_batting.find(\"table\", class_=\"table\")\n",
    "rows_batting = table_batting.find_all(\"tr\")\n",
    "\n",
    "for row in rows_batting[1:11]:\n",
    "  player_name = row.find(\"td\", class_=\"table-body__cell rankings-table__name name\").text.strip()\n",
    "  team = row.find(\"span\", class_=\"table-body__logo-text\").text.strip()\n",
    "  rating = row.find(\"td\", class_=\"table-body__cell rating\").text.strip()\n",
    "  batting_data.append([player_name, team, rating])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafabf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans:- C) \n",
    "\n",
    "# Scrape Top 10 women's ODI all-rounders\n",
    "url_allrounders = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response_allrounders = requests.get(url_allrounders)\n",
    "soup_allrounders = BeautifulSoup(response_allrounders.content, \"html.parser\")\n",
    "\n",
    "allrounders_data = []\n",
    "table_allrounders = soup_allrounders.find(\"table\", class_=\"table\")\n",
    "rows_allrounders = table_allrounders.find_all(\"tr\")\n",
    "\n",
    "for row in rows_allrounders[1:11]:\n",
    "  player_name = row.find(\"td\", class_=\"table-body__cell rankings-table__name name\").text.strip()\n",
    "  team = row.find(\"span\", class_=\"table-body__logo-text\").text.strip()\n",
    "  rating = row.find(\"td\", class_=\"table-body__cell rating\").text.strip()\n",
    "  allrounders_data.append([player_name, team, rating])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frames\n",
    "df_teams = pd.DataFrame(teams_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "df_batting = pd.DataFrame(batting_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "df_allrounders = pd.DataFrame(allrounders_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "\n",
    "# Print the data frames\n",
    "print(\"Top 10 ODI teams in women's cricket:\")\n",
    "print(df_teams)\n",
    "print(\"\\nTop 10 women's ODI Batting players:\")\n",
    "print(df_batting)\n",
    "print(\"\\nTop 10 women's ODI all-rounders:\")\n",
    "print(df_allrounders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1db166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:- 5 Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news articles on the page\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "# Loop through each article and extract the required information\n",
    "for article in articles:\n",
    "  # Extract the headline\n",
    "  headline = article.find(\"a\").text.strip()\n",
    "  headlines.append(headline)\n",
    "  \n",
    "  # Extract the time\n",
    "  time = article.find(\"time\").text.strip()\n",
    "  times.append(time)\n",
    "  \n",
    "  # Extract the news link\n",
    "  link = article.find(\"a\")[\"href\"]\n",
    "  links.append(link)\n",
    "\n",
    "# Create a dataframe using the scraped data\n",
    "data = {\n",
    "  \"Headline\": headlines,\n",
    "  \"Time\": times,\n",
    "  \"News Link\": links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:- 6 Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de81417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container that holds the article details\n",
    "articles_container = soup.find(\"div\", class_=\"pod-listing\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "# Iterate over each article in the container\n",
    "for article in articles_container.find_all(\"li\"):\n",
    "  # Scrape the title\n",
    "  title = article.find(\"h3\").text.strip()\n",
    "  titles.append(title)\n",
    "  \n",
    "  # Scrape the authors\n",
    "  author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "  authors.append(author)\n",
    "  \n",
    "  # Scrape the published date\n",
    "  date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "  dates.append(date)\n",
    "  \n",
    "  # Scrape the paper URL\n",
    "  url = article.find(\"a\")[\"href\"]\n",
    "  urls.append(url)\n",
    "\n",
    "# Create a dataframe with the scraped data\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c869e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question:-7 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac8cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the elements containing the details you want to scrape\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "# Create empty lists to store the scraped data\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "# Extract the data from the elements and append them to the respective lists\n",
    "for name in restaurant_names:\n",
    "  restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "  cuisine_list.append(cuisine.text.strip())\n",
    "\n",
    "for location in locations:\n",
    "  location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "  rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "  image_url_list.append(image['src'])\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
