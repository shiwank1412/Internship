{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a678fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Enter search criteria\n",
    "job_title = \"Data Analyst\"\n",
    "location = \"Bangalore\"\n",
    "\n",
    "# Step 3: Perform search\n",
    "search_url = f\"{url}/job-search/{job_title}-jobs-in-{location}\"\n",
    "response = requests.get(search_url)\n",
    "\n",
    "# Step 4: Scrape data for the first 10 jobs\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "job_results = soup.find_all(\"li\", class_=\"search_listing\")\n",
    "\n",
    "data = []\n",
    "for job in job_results[:10]:\n",
    "  title = job.find(\"h2\").text.strip()\n",
    "  company = job.find(\"span\", class_=\"company_name\").text.strip()\n",
    "  location = job.find(\"span\", class_=\"location\").text.strip()\n",
    "  experience = job.find(\"span\", class_=\"exp\").text.strip()\n",
    "\n",
    "  data.append({\n",
    "  \"Job Title\": title,\n",
    "  \"Company Name\": company,\n",
    "  \"Job Location\": location,\n",
    "  \"Experience Required\": experience\n",
    "  })\n",
    "\n",
    "# Step 5: Create a dataframe of the scraped data\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c42c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "search_form = soup.find(\"form\", {\"id\": \"searchForm\"})\n",
    "job_title_field = search_form.find(\"input\", {\"id\": \"txt_search\"})\n",
    "location_field = search_form.find(\"input\", {\"id\": \"txt_location\"})\n",
    "\n",
    "job_title_field[\"value\"] = \"Data Scientist\"\n",
    "location_field[\"value\"] = \"Bangalore\"\n",
    "\n",
    "search_button = search_form.find(\"button\", {\"type\": \"submit\"})\n",
    "search_url = url + search_button[\"formaction\"]\n",
    "response = requests.post(search_url, data=search_form.serialize())\n",
    "\n",
    "job_results = soup.find_all(\"div\", {\"class\": \"search_listing\"})\n",
    "job_data = []\n",
    "\n",
    "for job in job_results[:10]:\n",
    "  title = job.find(\"a\", {\"class\": \"job_title\"}).text.strip()\n",
    "  location = job.find(\"span\", {\"class\": \"job_location\"}).text.strip()\n",
    "  company = job.find(\"a\", {\"class\": \"company_name\"}).text.strip()\n",
    "  job_data.append({\"Job Title\": title, \"Location\": location, \"Company\": company})\n",
    "\n",
    "df = pd.DataFrame(job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage\n",
    " You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scrapeddata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "search_input = soup.find(\"input\", {\"id\": \"txt_search\"})\n",
    "search_input[\"value\"] = \"Data Scientist\"\n",
    "\n",
    "search_button = soup.find(\"button\", {\"id\": \"btn_search\"})\n",
    "response = requests.post(url, data={\"txt_search\": \"Data Scientist\"})\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "location_filter = soup.find(\"input\", {\"id\": \"chk_location_1\"})\n",
    "location_filter[\"checked\"] = True\n",
    "\n",
    "salary_filter = soup.find(\"input\", {\"id\": \"chk_salary_1\"})\n",
    "salary_filter[\"checked\"] = True\n",
    "\n",
    "job_results = soup.find_all(\"div\", {\"class\": \"search_listing\"})\n",
    "data = []\n",
    "\n",
    "for job in job_results[:10]:\n",
    "  job_title = job.find(\"h3\").text.strip()\n",
    "  job_location = job.find(\"span\", {\"class\": \"location\"}).text.strip()\n",
    "  company_name = job.find(\"span\", {\"class\": \"company_name\"}).text.strip()\n",
    "  experience_required = job.find(\"span\", {\"class\": \"exp\"}).text.strip()\n",
    "  \n",
    "  data.append({\n",
    "  \"Job Title\": job_title,\n",
    "  \"Job Location\": job_location,\n",
    "  \"Company Name\": company_name,\n",
    "  \"Experience Required\": experience_required\n",
    "  })\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "\n",
    " 6. Brand\n",
    "\n",
    " 7. ProductDescription \n",
    "\n",
    "8. Price The attributes which you have to scrape is ticked marked in the below image.\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    " 1. Go to Flipkart webpage by url :https://www.flipkart.com/ \n",
    "\n",
    "2. Enter “sunglasses” in the search fieldwhere “search for products, brands and more” is written and click the search icon \n",
    "\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual. \n",
    "\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then click on it.\n",
    "\n",
    " 5. Now scrape data from this page as usual\n",
    "\n",
    " 6. Repeat this until you get data for 100sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.flipkart.com/\"\n",
    "search_query = \"sunglasses\"\n",
    "max_listings = 100\n",
    "scraped_data = []\n",
    "\n",
    "while len(scraped_data) < max_listings:\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "  \n",
    "  # Find the search field and enter the search query\n",
    "  search_field = soup.find(\"input\", attrs={\"title\": \"Search for products, brands and more\"})\n",
    "  search_field[\"value\"] = search_query\n",
    "  \n",
    "  # Click the search icon\n",
    "  search_icon = soup.find(\"button\", attrs={\"type\": \"submit\"})\n",
    "  response = requests.post(url, data=search_icon.form)\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "  \n",
    "  # Find the container holding the listings\n",
    "  listings_container = soup.find(\"div\", attrs={\"class\": \"_1AtVbE\"})\n",
    "  \n",
    "  # Extract the required attributes from each listing\n",
    "  for listing in listings_container.find_all(\"div\", attrs={\"class\": \"_2kHMtA\"}):\n",
    "  brand = listing.find(\"div\", attrs={\"class\": \"_2WkVRV\"}).text\n",
    "  description = listing.find(\"a\", attrs={\"class\": \"IRpwTa\"}).text\n",
    "  price = listing.find(\"div\", attrs={\"class\": \"_30jeq3 _1_WHN1\"}).text\n",
    "  \n",
    "  scraped_data.append({\"Brand\": brand, \"ProductDescription\": description, \"Price\": price})\n",
    "  \n",
    "  if len(scraped_data) == max_listings:\n",
    "  break\n",
    "  \n",
    "  # Find the \"Next\" button and navigate to the next page\n",
    "  next_button = soup.find(\"a\", attrs={\"class\": \"_1LKTO3\"})\n",
    "  if next_button:\n",
    "  url = \"https://www.flipkart.com\" + next_button[\"href\"]\n",
    "  else:\n",
    "  break\n",
    "\n",
    "# Print the scraped data\n",
    "for data in scraped_data:\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be17962",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5:- Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the\n",
    "link: https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pidMOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market place=FLIPKART  As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55f893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed384cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first connect to the web driver\n",
    "driver = webdriver.Chrome(r\"C:\\Users\\xyz\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g\n",
    "\n",
    "# write on search bar\n",
    "search_g.send_keys('sneakers')\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn\n",
    "\n",
    "search_btn=driver.find_element_by_class_name('L0Z3Pu')\n",
    "search_btn.click()\n",
    "\n",
    "B_name=[]\n",
    "Price=[]\n",
    "P_desc=[]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    \n",
    "     for j  in b_name:\n",
    "        B_name.append(j.text)\n",
    "    B_name[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    \n",
    "    \n",
    "    print(len(B_name[:100])),print(len(Price[:100])),print(len(P_desc[:100]))\n",
    "    \n",
    "    sun_gl=pd.DataFrame({})\n",
    "sun_gl['Brand_name']=B_name[:100]\n",
    "sun_gl['P_price']=Price[:100]\n",
    "sun_gl['Pr_desc']=P_desc[:100]\n",
    "\n",
    "sun_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e83701",
   "metadata": {},
   "outputs": [],
   "source": [
    "q7: go to webpage https://www.amazon.in/ enter “laptop” in the search field and then click the search icon. then set cpu type filter to “intel core i7” as shown in the below image: After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71dd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\" https://www.amazon.in \"\n",
    "driver.get(url)\n",
    "\n",
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g\n",
    "\n",
    "# write on search bar\n",
    "search_g.send_keys('Laptop')\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "search_btn\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "search_btn.click()\n",
    "\n",
    "Title=[]\n",
    "Price=[]\n",
    "Rating=[]\n",
    "\n",
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    \n",
    "    \n",
    "    for j  in b_name:\n",
    "        Title.append(j.text)\n",
    "    Title[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0a21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8: Write a python program to scrape data for Top 1000 Quotes of All Time. The above task will be done in following steps: \n",
    "1. First get the webpagehttps://www.azquotes.com/ \n",
    "2. Click on TopQuotes \n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecdb241",
   "metadata": {},
   "outputs": [],
   "source": [
    "To scrape data for the top 1000 quotes of all time from the website \"https://www.azquotes.com/\", you can follow these steps:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "Set up the Chrome driver:\n",
    "driver = webdriver.Chrome()\n",
    "#Open the webpage:\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "Click on \"Top Quotes\":\n",
    "top_quotes_button = driver.find_element(By.LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_button.click()\n",
    "#Scrape the data:\n",
    "quotes = driver.find_elements(By.CSS_SELECTOR, \".title a\")\n",
    "authors = driver.find_elements(By.CSS_SELECTOR, \".author a\")\n",
    "types = driver.find_elements(By.CSS_SELECTOR, \".kw-box a\")\n",
    "\n",
    "for quote, author, quote_type in zip(quotes, authors, types):\n",
    "print(\"Quote:\", quote.text)\n",
    "print(\"Author:\", author.text)\n",
    "print(\"Type of Quote:\", quote_type.text)\n",
    "print()\n",
    "#Close the browser:\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e203ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries:\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "#Set up the Selenium webdriver:\n",
    "driver = webdriver.Chrome('path_to_chromedriver')\n",
    "#Open the webpage:\n",
    "driver.get('https://www.jagranjosh.com/')\n",
    "#Click on the \"GK\" option:\n",
    "gk_option = driver.find_element_by_link_text('GK')\n",
    "gk_option.click()\n",
    "#Click on the \"List of all Prime Ministers of India\":\n",
    "pm_option = driver.find_element_by_link_text('List of all Prime Ministers of India')\n",
    "pm_option.click()\n",
    "# Scrape the data:\n",
    "data = []\n",
    "table = driver.find_element_by_xpath('//table[@class=\"table4\"]')\n",
    "rows = table.find_elements_by_tag_name('tr')\n",
    "for row in rows:\n",
    "  cols = row.find_elements_by_tag_name('td')\n",
    "  if len(cols) == 4:\n",
    "  name = cols[0].text\n",
    "  born_dead = cols[1].text\n",
    "  term_of_office = cols[2].text\n",
    "  remarks = cols[3].text\n",
    "  data.append([name, born_dead, term_of_office, remarks])\n",
    "#Create a DataFrame:\n",
    "df = pd.DataFrame(data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "# Close the Webdriver:\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65c6e9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Get the webpage\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver = webdriver.Chrome('path_to_chromedriver')  # Replace 'path_to_chromedriver' with the actual path to your ChromeDriver executable\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "# Step 2: Type in the search bar\n",
    "search_bar = driver.find_element_by_id('search-input')\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.submit()\n",
    "\n",
    "# Step 3: Click on the link\n",
    "link = driver.find_element_by_link_text('50 Most Expensive Cars in the World')\n",
    "link.click()\n",
    "\n",
    "# Step 4: Scrape the data and create a dataframe\n",
    "car_names = driver.find_elements_by_xpath('//div[@class=\"article-content\"]/h3')\n",
    "car_prices = driver.find_elements_by_xpath('//div[@class=\"article-content\"]/p')\n",
    "\n",
    "data = []\n",
    "for name, price in zip(car_names, car_prices):\n",
    "  data.append([name.text, price.text])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Car Name', 'Price'])\n",
    "print(df)\n",
    "\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
