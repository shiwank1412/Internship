{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77554b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url\n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "A)Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6565232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1.\n",
      "Artist: Pinkfong Baby Shark - Kids' Songs & Stories\n",
      "Upload Date: June 17, 2016\n",
      "Views: \"Baby Shark Dance\"[6]\n",
      "Rank: 2.\n",
      "Artist: Luis Fonsi\n",
      "Upload Date: January 12, 2017\n",
      "Views: \"Despacito\"[9]\n",
      "Rank: 3.\n",
      "Artist: LooLoo Kids - Nursery Rhymes and Children's Songs\n",
      "Upload Date: October 8, 2016\n",
      "Views: \"Johny Johny Yes Papa\"[17]\n",
      "Rank: 4.\n",
      "Artist: Cocomelon - Nursery Rhymes\n",
      "Upload Date: May 2, 2018\n",
      "Views: \"Bath Song\"[18]\n",
      "Rank: 5.\n",
      "Artist: Ed Sheeran\n",
      "Upload Date: January 30, 2017\n",
      "Views: \"Shape of You\"[19]\n",
      "Rank: 6.\n",
      "Artist: Wiz Khalifa\n",
      "Upload Date: April 6, 2015\n",
      "Views: \"See You Again\"[22]\n",
      "Rank: 7.\n",
      "Artist: Cocomelon - Nursery Rhymes\n",
      "Upload Date: May 24, 2018\n",
      "Views: \"Wheels on the Bus\"[27]\n",
      "Rank: 8.\n",
      "Artist: ChuChu TV Nursery Rhymes & Kids Songs\n",
      "Upload Date: March 6, 2014\n",
      "Views: \"Phonics Song with Two Words\"[28]\n",
      "Rank: 9.\n",
      "Artist: Mark Ronson\n",
      "Upload Date: November 19, 2014\n",
      "Views: \"Uptown Funk\"[29]\n",
      "Rank: 10.\n",
      "Artist: Miroshka TV\n",
      "Upload Date: February 27, 2018\n",
      "Views: \"Learning Colors – Colorful Eggs on a Farm\"[30]\n",
      "Rank: 11.\n",
      "Artist: officialpsy\n",
      "Upload Date: July 15, 2012\n",
      "Views: \"Gangnam Style\"[31]\n",
      "Rank: 12.\n",
      "Artist: Get Movies\n",
      "Upload Date: January 31, 2012\n",
      "Views: \"Masha and the Bear – Recipe for Disaster\"[36]\n",
      "Rank: 13.\n",
      "Artist: Ultra Records\n",
      "Upload Date: April 5, 2018\n",
      "Views: \"Dame Tu Cosita\"[37]\n",
      "Rank: 14.\n",
      "Artist: Crazy Frog\n",
      "Upload Date: June 16, 2009\n",
      "Views: \"Axel F\"[38]\n",
      "Rank: 15.\n",
      "Artist: Maroon 5\n",
      "Upload Date: January 14, 2015\n",
      "Views: \"Sugar\"[39]\n",
      "Rank: 16.\n",
      "Artist: OneRepublic\n",
      "Upload Date: May 31, 2013\n",
      "Views: \"Counting Stars\"[40]\n",
      "Rank: 17.\n",
      "Artist: Katy Perry\n",
      "Upload Date: September 5, 2013\n",
      "Views: \"Roar\"[41]\n",
      "Rank: 18.\n",
      "Artist: Cocomelon - Nursery Rhymes\n",
      "Upload Date: June 25, 2018\n",
      "Views: \"Baa Baa Black Sheep\"[42]\n",
      "Rank: 19.\n",
      "Artist: Shakira\n",
      "Upload Date: June 4, 2010\n",
      "Views: \"Waka Waka (This Time for Africa)\"[43]\n",
      "Rank: 20.\n",
      "Artist: Jingle Toons\n",
      "Upload Date: June 14, 2018\n",
      "Views: \"Lakdi Ki Kathi\"[44]\n",
      "Rank: 21.\n",
      "Artist: Justin Bieber\n",
      "Upload Date: October 22, 2015\n",
      "Views: \"Sorry\"[45]\n",
      "Rank: 22.\n",
      "Artist: Ed Sheeran\n",
      "Upload Date: October 7, 2014\n",
      "Views: \"Thinking Out Loud\"[46]\n",
      "Rank: 23.\n",
      "Artist: Kiddiestv Hindi - Nursery Rhymes & Kids Songs\n",
      "Upload Date: January 26, 2018\n",
      "Views: \"Humpty the train on a fruits ride\"[47]\n",
      "Rank: 24.\n",
      "Artist: Katy Perry\n",
      "Upload Date: February 20, 2014\n",
      "Views: \"Dark Horse\"[48]\n",
      "Rank: 25.\n",
      "Artist: Ed Sheeran\n",
      "Upload Date: November 9, 2017\n",
      "Views: \"Perfect\"[49]\n",
      "Rank: 26.\n",
      "Artist: Passenger\n",
      "Upload Date: July 25, 2012\n",
      "Views: \"Let Her Go\"[50]\n",
      "Rank: 27.\n",
      "Artist: Alan Walker\n",
      "Upload Date: December 3, 2015\n",
      "Views: \"Faded\"[51]\n",
      "Rank: 28.\n",
      "Artist: T-Series Bhakti Sagar\n",
      "Upload Date: May 10, 2011\n",
      "Views: \"Shree Hanuman Chalisa\"[52]\n",
      "Rank: 29.\n",
      "Artist: Maroon 5\n",
      "Upload Date: May 31, 2018\n",
      "Views: \"Girls Like You\"[53]\n",
      "Rank: 30.\n",
      "Artist: Major Lazer Official\n",
      "Upload Date: March 22, 2015\n",
      "Views: \"Lean On\"[54]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     13\u001b[0m   cells \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m   rank \u001b[38;5;241m=\u001b[39m \u001b[43mcells\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     15\u001b[0m   artist \u001b[38;5;241m=\u001b[39m cells[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m   upload_date \u001b[38;5;241m=\u001b[39m cells[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  rank = cells[0].text.strip()\n",
    "  artist = cells[2].text.strip()\n",
    "  upload_date = cells[4].text.strip()\n",
    "  views = cells[1].text.strip()\n",
    "  \n",
    "  # Do something with the extracted details (e.g., print them)\n",
    "  print(f\"Rank: {rank}\")\n",
    "  print(f\"Artist: {artist}\")\n",
    "  print(f\"Upload Date: {upload_date}\")\n",
    "  print(f\"Views: {views}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400de92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details: \n",
    "A) Series\n",
    "B) Place\n",
    "C) Date\n",
    "D) Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87db3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the international fixtures page\n",
    "url = \"https://www.bcci.tv/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the elements containing the fixture details\n",
    "fixture_elements = soup.find_all(\"div\", class_=\"fixture__format-strip\")\n",
    "\n",
    "# Extract the details and store them in a data structure\n",
    "fixtures = []\n",
    "for element in fixture_elements:\n",
    "  series = element.find(\"span\", class_=\"fixture__format-strip--suffix\").text.strip()\n",
    "  place = element.find(\"p\", class_=\"fixture__additional-info\").text.strip()\n",
    "  date = element.find(\"span\", class_=\"fixture__datetime\").text.strip()\n",
    "  time = element.find(\"span\", class_=\"fixture__time\").text.strip()\n",
    "\n",
    "  fixture = {\n",
    "  \"Series\": series,\n",
    "  \"Place\": place,\n",
    "  \"Date\": date,\n",
    "  \"Time\": time\n",
    "  }\n",
    "  fixtures.append(fixture)\n",
    "\n",
    "# Print the extracted details\n",
    "for fixture in fixtures:\n",
    "  print(fixture)\n",
    "\n",
    "pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)- at current prices\n",
    "D) GSDP(19-20)- at current prices\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cef460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send GET request to the home page\n",
    "url = \"http://statisticstimes.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the link to the \"Economy\" page\n",
    "economy_link = soup.select_one(\"a[href*='economy']\")[\"href\"]\n",
    "\n",
    "# Send GET request to the \"Economy\" page\n",
    "economy_url = url + economy_link\n",
    "economy_response = requests.get(economy_url)\n",
    "economy_soup = BeautifulSoup(economy_response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the State-wise GDP details\n",
    "gdp_table = economy_soup.select_one(\"table#table_id\")\n",
    "\n",
    "# Extract the required details from the table\n",
    "data = []\n",
    "rows = gdp_table.select(\"tbody tr\")\n",
    "for row in rows:\n",
    "  cells = row.select(\"td\")\n",
    "  rank = cells[0].text.strip()\n",
    "  state = cells[1].text.strip()\n",
    "  gdp_18_19 = cells[2].text.strip()\n",
    "  gdp_19_20 = cells[3].text.strip()\n",
    "  share_18_19 = cells[4].text.strip()\n",
    "  gdp_billion = cells[5].text.strip()\n",
    "  data.append({\n",
    "  \"Rank\": rank,\n",
    "  \"State\": state,\n",
    "  \"GSDP(18-19)\": gdp_18_19,\n",
    "  \"GSDP(19-20)\": gdp_19_20,\n",
    "  \"Share(18-19)\": share_18_19,\n",
    "  \"GDP($ billion)\": gdp_billion\n",
    "  })\n",
    "\n",
    "# Print the extracted data\n",
    "for item in data:\n",
    "  print(item)\n",
    "\n",
    "pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31182272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1038e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the\n",
    "following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Jaimes Subroto's Billboard Hot 100 Python Web Scraper!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen as uRequest\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "url = 'https://www.billboard.com/charts/hot-100'\n",
    "\n",
    "# Opening up connection, grabbing the page\n",
    "uClient = uRequest(url)\n",
    "page_html = uClient.read() # Offloads content into a variable\n",
    "uClient.close() # Close the client\n",
    "\n",
    "# HTML parsing\n",
    "page_soup = soup(page_html, \"html.parser\")\n",
    "\n",
    "# Grabs all information related to the top 100 songs\n",
    "containers = page_soup.select('article[class*=chart]') # *= means contains\n",
    "\n",
    "filename = 'billboard_hot_100.csv'\n",
    "f = open(filename, 'w') # w = write\n",
    "\n",
    "headers = 'Song, Artist, Last Week, Peak Position, Weeks on Chart\\n'\n",
    "\n",
    "f.write(headers)\n",
    "\n",
    "print('Welcome to Jaimes Subroto\\'s Billboard Hot 100 Python Web Scraper!')\n",
    "\n",
    "# Asks the user if he/she wants the data to be printed to the console\n",
    "while True:\n",
    "    print_data = input('Would you like the data to be printed to the console? ')\n",
    "    if print_data.lower() == 'yes' or print_data.lower() == 'y':\n",
    "        print_data = True\n",
    "        print('\\nPrinting this week\\'s HOTTEST 100 songs...')\n",
    "        break\n",
    "    elif print_data.lower() == 'no' or print_data.lower() == 'n':\n",
    "        print_data = False\n",
    "        print('\\nScraping this week\\'s HOTTEST 100 songs...')\n",
    "        break\n",
    "    else:\n",
    "        print('Sorry, I didn\\'t get that.')\n",
    "\n",
    "chart_position = 1\n",
    "\n",
    "# Loops through each container\n",
    "for container in containers:\n",
    "\n",
    "    # Container storing the song name and artist name\n",
    "    song_container = container.find('div', {'class': 'chart-row__title'})\n",
    "\n",
    "    # Grabs the song name\n",
    "    song = song_container.h2.text\n",
    "    \n",
    "    # Grabs the artist name\n",
    "    try:\n",
    "        artist = song_container.a.text.strip()\n",
    "    except AttributeError:\n",
    "        artist = song_container.span.text.strip()\n",
    "\n",
    "    # Grabs the song's position last week\n",
    "    last_week_container = container.find('div', {'class': 'chart-row__last-week'})\n",
    "    last_week = last_week_container.find('span', {'class': 'chart-row__value'}).text\n",
    "\n",
    "    # Grabs the song's peak position\n",
    "    peak_position_container = container.find('div', {'class': 'chart-row__top-spot'})\n",
    "    peak_position = peak_position_container.find('span', {'class': 'chart-row__value'}).text\n",
    "\n",
    "    # Grabs the song's duration in the hot 100 (in weeks)\n",
    "    weeks_on_chart_container = container.find('div', {'class': 'chart-row__weeks-on-chart'})\n",
    "    weeks_on_chart = weeks_on_chart_container.find('span', {'class': 'chart-row__value'}).text\n",
    "\n",
    "    # Prints the chart position, song name, artist name, and related stats\n",
    "    if print_data:\n",
    "        print('\\nPosition: #{}'.format(chart_position))\n",
    "        print('Song: {}'.format(song))\n",
    "        print('Artist: {}'.format(artist))\n",
    "        print('Last Week: {}'.format(last_week))\n",
    "        print('Peak Position: {}'.format(peak_position))\n",
    "        print('Weeks on Chart: {}'.format(weeks_on_chart))\n",
    "\n",
    "    chart_position += 1\n",
    "\n",
    "    f.write('\\\"' + song + '\\\",\\\"' + artist.replace('Featuring', 'Feat.') + '\\\",' + last_week + ',' + peak_position + ',' + weeks_on_chart + '\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('\\nWeb scraped data saved to {}'.format(filename))\n",
    "print('Thank you for using Jaimes Subroto\\'s Web Scraping app for Billboard HOT 100!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ceb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Scrape the details of Highest selling novels.\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8c27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/ You have\n",
    "to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a356053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lxml\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "\n",
    "url1 = \"https://www.imdb.com/search/title?count=100&title_type=feature,tv_series&ref_=nv_wl_img_2\"\n",
    "\n",
    "class IMDB(object):\n",
    "\"\"\"docstring for IMDB\"\"\"\n",
    "def __init__(self, url):\n",
    "super(IMDB, self).__init__()\n",
    "page = get(url)\n",
    "\n",
    "self.soup = BeautifulSoup(page.content, 'lxml')\n",
    "\n",
    "def articleTitle(self):\n",
    "return self.soup.find(\"h1\", class_=\"header\").text.replace(\"\\n\",\"\")\n",
    "\n",
    "def bodyContent(self):\n",
    "content = self.soup.find(id=\"main\")\n",
    "return content.find_all(\"div\", class_=\"lister-item mode-advanced\")\n",
    "\n",
    "def movieData(self):\n",
    "movieName = self.bodyContent()\n",
    "movieRunTime = []\n",
    "movieGenre = []\n",
    "movieRating = []\n",
    "movieVotes = []\n",
    "movieYearspan = []\n",
    "\n",
    "movieName.append(movieFirstLine.find(\"a\").text)\n",
    "try:\n",
    "movieRunTime.append(movie.find(\"span\", class_=\"runtime\").text[:-4])\n",
    "except:\n",
    "movieRunTime.append(np.nan)\n",
    "movieGenre.append(movie.find(\"span\", class_=\"genre\").text.rstrip().replace(\"\\n\",\"\").split(\",\"))\n",
    "try:\n",
    "movieRating.append(movie.find(\"strong\").text)\n",
    "except:\n",
    "movieRating.append(np.nan)\n",
    "try:\n",
    "movieYearspan.append(movie.find(\"span\", class_=\"metascore unfavorable\").text.rstrip())\n",
    "except:\n",
    "Yearspan.append(np.nan)\n",
    "\n",
    "\n",
    "movieNumbers = movie.find_all(\"span\", attrs={\"name\": \"nv\"})\n",
    "\n",
    "if len(movieNumbers) == 2:\n",
    "movieVotes.append(movieNumbers[0].text)\n",
    "\n",
    "elif len(movieNumbers) == 1:\n",
    "movieVotes.append(movieNumbers[0].text)\n",
    "\n",
    "else:\n",
    "movieVotes.append(np.nan)\n",
    "\n",
    "\n",
    "movieData = [movieTitle, movieDate, movieRunTime, movieGenre, movieRating, movieScore, movieDescription,\n",
    "movieDirector, movieStars, movieVotes, movieGross]\n",
    "return movieData\n",
    "    \n",
    "    id1 = IMDB(url1)\n",
    "#Get Article Title\n",
    "print(id1.articleTitle())\n",
    "#Get the first 5 movie data using for loop\n",
    "for i in range(5):\n",
    "\tprint(movieData[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Details of Datasets from UCI machine learning repositories.\n",
    "Url = https://archive.ics.uci.edu/ You\n",
    "have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e52fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
